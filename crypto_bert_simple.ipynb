{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîê Simple Crypto-BERT for Colab\n",
    "## Optimized for Google Colab Free Tier\n",
    "\n",
    "This version avoids mixed precision issues and uses a simplified architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install -q tensorflow==2.13.0\n",
    "!pip install -q scikit-learn matplotlib seaborn tqdm\n",
    "\n",
    "# Simple setup without mixed precision\n",
    "import tensorflow as tf\n",
    "\n",
    "# Enable memory growth for GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"üöÄ GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple configuration\n",
    "class SimpleConfig:\n",
    "    vocab_size = 10000\n",
    "    max_length = 128\n",
    "    embedding_dim = 256\n",
    "    num_heads = 8\n",
    "    ff_dim = 512\n",
    "    num_transformer_blocks = 4\n",
    "    dropout = 0.1\n",
    "    num_classes = 15\n",
    "    batch_size = 32\n",
    "    epochs = 5\n",
    "    learning_rate = 1e-3\n",
    "    samples_per_class = 600\n",
    "    \n",
    "    class_labels = [\n",
    "        'STRONG_ENCRYPTION', 'WEAK_CIPHER_SUITE', 'CERTIFICATE_INVALID',\n",
    "        'KEY_REUSE', 'DOWNGRADE_ATTACK', 'MAN_IN_MIDDLE', 'REPLAY_ATTACK',\n",
    "        'TIMING_ATTACK', 'QUANTUM_VULNERABLE', 'ENTROPY_WEAKNESS',\n",
    "        'HASH_COLLISION', 'PADDING_ORACLE', 'LENGTH_EXTENSION',\n",
    "        'PROTOCOL_CONFUSION', 'CRYPTO_AGILITY_LACK'\n",
    "    ]\n",
    "\n",
    "config = SimpleConfig()\n",
    "print(f\"üìã Config loaded: {config.samples_per_class * config.num_classes:,} total samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple data generator\n",
    "def generate_crypto_data(config):\n",
    "    \"\"\"Generate simple protocol sequences\"\"\"\n",
    "    \n",
    "    templates = {\n",
    "        'STRONG_ENCRYPTION': ['tls13 aes256 gcm secure', 'wpa3 chacha20 strong'],\n",
    "        'WEAK_CIPHER_SUITE': ['tls10 rc4 weak', 'wpa des vulnerable'],\n",
    "        'CERTIFICATE_INVALID': ['cert expired invalid', 'ssl chain broken'],\n",
    "        'KEY_REUSE': ['key reuse detected', 'nonce repeated'],\n",
    "        'DOWNGRADE_ATTACK': ['tls downgrade detected', 'version rollback'],\n",
    "        'MAN_IN_MIDDLE': ['cert mismatch mitm', 'dns spoofing'],\n",
    "        'REPLAY_ATTACK': ['message replay detected', 'timestamp invalid'],\n",
    "        'TIMING_ATTACK': ['timing variation high', 'side channel'],\n",
    "        'QUANTUM_VULNERABLE': ['rsa quantum weak', 'post quantum needed'],\n",
    "        'ENTROPY_WEAKNESS': ['entropy low weak', 'predictable random'],\n",
    "        'HASH_COLLISION': ['hash collision detected', 'md5 vulnerable'],\n",
    "        'PADDING_ORACLE': ['padding oracle detected', 'cbc padding'],\n",
    "        'LENGTH_EXTENSION': ['length extension possible', 'hash extend'],\n",
    "        'PROTOCOL_CONFUSION': ['protocol confusion', 'mixed implementation'],\n",
    "        'CRYPTO_AGILITY_LACK': ['crypto agility limited', 'single cipher']\n",
    "    }\n",
    "    \n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    print(\"üîÑ Generating data...\")\n",
    "    for class_idx, class_name in enumerate(tqdm(config.class_labels)):\n",
    "        for _ in range(config.samples_per_class):\n",
    "            template = random.choice(templates[class_name])\n",
    "            # Add some variation\n",
    "            if random.random() < 0.3:\n",
    "                template += f\" session {random.randint(1000, 9999)}\"\n",
    "            sequences.append(template)\n",
    "            labels.append(class_idx)\n",
    "    \n",
    "    return sequences, labels\n",
    "\n",
    "sequences, labels = generate_crypto_data(config)\n",
    "print(f\"‚úÖ Generated {len(sequences):,} sequences\")\n",
    "print(f\"Sample: {sequences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tokenizer\n",
    "def create_tokenizer(sequences, vocab_size, max_length):\n",
    "    \"\"\"Create simple word-based tokenizer\"\"\"\n",
    "    \n",
    "    # Special tokens\n",
    "    word_to_id = {'<PAD>': 0, '<UNK>': 1}\n",
    "    \n",
    "    # Count words\n",
    "    word_freq = {}\n",
    "    for seq in sequences:\n",
    "        for word in seq.lower().split():\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    \n",
    "    # Add most frequent words\n",
    "    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for word, freq in sorted_words[:vocab_size-2]:\n",
    "        word_to_id[word] = len(word_to_id)\n",
    "    \n",
    "    # Encode sequences\n",
    "    encoded = []\n",
    "    for seq in tqdm(sequences, desc=\"Encoding\"):\n",
    "        words = seq.lower().split()[:max_length]\n",
    "        ids = [word_to_id.get(word, word_to_id['<UNK>']) for word in words]\n",
    "        \n",
    "        # Pad\n",
    "        while len(ids) < max_length:\n",
    "            ids.append(word_to_id['<PAD>'])\n",
    "        \n",
    "        encoded.append(ids[:max_length])\n",
    "    \n",
    "    return np.array(encoded), word_to_id\n",
    "\n",
    "X, tokenizer = create_tokenizer(sequences, config.vocab_size, config.max_length)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(f\"üìù Encoded shape: {X.shape}\")\n",
    "print(f\"üî§ Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "# Clean up\n",
    "del sequences, labels\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, stratify=y_train, random_state=SEED\n",
    ")\n",
    "\n",
    "# Convert to categorical\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train, config.num_classes)\n",
    "y_val_cat = tf.keras.utils.to_categorical(y_val, config.num_classes)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, config.num_classes)\n",
    "\n",
    "print(f\"üìä Train: {X_train.shape[0]:,}, Val: {X_val.shape[0]:,}, Test: {X_test.shape[0]:,}\")\n",
    "\n",
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple transformer model using Keras built-in layers\n",
    "def create_simple_bert(config):\n",
    "    \"\"\"Create simplified BERT using Keras MultiHeadAttention\"\"\"\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(config.max_length,))\n",
    "    \n",
    "    # Embedding\n",
    "    x = tf.keras.layers.Embedding(\n",
    "        config.vocab_size, config.embedding_dim, mask_zero=True\n",
    "    )(inputs)\n",
    "    \n",
    "    # Positional encoding (simple)\n",
    "    positions = tf.keras.layers.Embedding(\n",
    "        config.max_length, config.embedding_dim\n",
    "    )(tf.range(config.max_length))\n",
    "    \n",
    "    x = x + positions\n",
    "    \n",
    "    # Transformer blocks\n",
    "    for i in range(config.num_transformer_blocks):\n",
    "        # Multi-head attention (built-in Keras layer)\n",
    "        attn_out = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=config.num_heads,\n",
    "            key_dim=config.embedding_dim // config.num_heads,\n",
    "            dropout=config.dropout\n",
    "        )(x, x)\n",
    "        \n",
    "        x = tf.keras.layers.LayerNormalization()(x + attn_out)\n",
    "        \n",
    "        # Feed forward\n",
    "        ffn_out = tf.keras.layers.Dense(config.ff_dim, activation='gelu')(x)\n",
    "        ffn_out = tf.keras.layers.Dense(config.embedding_dim)(ffn_out)\n",
    "        ffn_out = tf.keras.layers.Dropout(config.dropout)(ffn_out)\n",
    "        \n",
    "        x = tf.keras.layers.LayerNormalization()(x + ffn_out)\n",
    "    \n",
    "    # Global average pooling instead of CLS token\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Classification head\n",
    "    x = tf.keras.layers.Dropout(config.dropout)(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='gelu')(x)\n",
    "    x = tf.keras.layers.Dropout(config.dropout)(x)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(config.num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs, name='SimpleCryptoBERT')\n",
    "    return model\n",
    "\n",
    "print(\"üèóÔ∏è Creating model...\")\n",
    "model = create_simple_bert(config)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nüìä Parameters: {total_params:,}\")\n",
    "print(f\"üìè Estimated size: {total_params * 4 / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(config.learning_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy', patience=2, restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=1, min_lr=1e-6\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"üöÄ Training...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    batch_size=config.batch_size,\n",
    "    epochs=config.epochs,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "\n",
    "print(f\"üìä Test Results:\")\n",
    "print(f\"   Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"   Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Plot training\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Val')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Val')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quick classification report\n",
    "y_pred = model.predict(X_test, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "print(f\"\\nüéØ Sample Class Accuracies:\")\n",
    "for i in range(min(5, config.num_classes)):\n",
    "    class_mask = y_true_classes == i\n",
    "    if np.sum(class_mask) > 0:\n",
    "        class_acc = np.mean(y_pred_classes[class_mask] == i)\n",
    "        print(f\"   {config.class_labels[i][:20]}: {class_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('crypto_bert_simple.h5')\n",
    "print(\"üíæ Model saved: crypto_bert_simple.h5\")\n",
    "\n",
    "# Save tokenizer\n",
    "with open('tokenizer_simple.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'word_to_id': tokenizer,\n",
    "        'vocab_size': len(tokenizer),\n",
    "        'max_length': config.max_length\n",
    "    }, f)\n",
    "print(\"üíæ Tokenizer saved: tokenizer_simple.json\")\n",
    "\n",
    "# Save config\n",
    "with open('config_simple.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'vocab_size': config.vocab_size,\n",
    "        'max_length': config.max_length,\n",
    "        'embedding_dim': config.embedding_dim,\n",
    "        'num_heads': config.num_heads,\n",
    "        'ff_dim': config.ff_dim,\n",
    "        'num_transformer_blocks': config.num_transformer_blocks,\n",
    "        'num_classes': config.num_classes,\n",
    "        'class_labels': config.class_labels,\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'total_parameters': int(total_params)\n",
    "    }, f, indent=2)\n",
    "print(\"üíæ Config saved: config_simple.json\")\n",
    "\n",
    "# Final stats\n",
    "import os\n",
    "file_size = os.path.getsize('crypto_bert_simple.h5') / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nüéâ Simple Crypto-BERT Complete!\")\n",
    "print(f\"   üìè File size: {file_size:.1f} MB\")\n",
    "print(f\"   üéØ Test accuracy: {test_acc:.4f}\")\n",
    "print(f\"   ‚ö° Fast training: ‚úÖ\")\n",
    "print(f\"   üß† Parameters: {total_params:,}\")\n",
    "\n",
    "print(f\"\\nüìÅ Files created:\")\n",
    "print(f\"   ‚Ä¢ crypto_bert_simple.h5\")\n",
    "print(f\"   ‚Ä¢ tokenizer_simple.json\")\n",
    "print(f\"   ‚Ä¢ config_simple.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files (if in Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"üì• Downloading files...\")\n",
    "    \n",
    "    files.download('crypto_bert_simple.h5')\n",
    "    files.download('tokenizer_simple.json')\n",
    "    files.download('config_simple.json')\n",
    "    \n",
    "    print(\"‚úÖ Download complete!\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è Files saved locally (not in Colab)\")\n",
    "\n",
    "print(\"\\nüéä Ready for integration with your CNN, GNN, and LSTM models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Usage Example\n",
    "\n",
    "```python\n",
    "# Load the model\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "model = tf.keras.models.load_model('crypto_bert_simple.h5')\n",
    "\n",
    "# Load tokenizer\n",
    "with open('tokenizer_simple.json', 'r') as f:\n",
    "    tokenizer_data = json.load(f)\n",
    "\n",
    "# Predict\n",
    "def predict_vulnerability(text):\n",
    "    # Tokenize\n",
    "    words = text.lower().split()[:128]  # max_length\n",
    "    ids = [tokenizer_data['word_to_id'].get(w, 1) for w in words]  # 1 = <UNK>\n",
    "    \n",
    "    # Pad\n",
    "    while len(ids) < 128:\n",
    "        ids.append(0)  # 0 = <PAD>\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.predict([ids])\n",
    "    return pred[0]\n",
    "```\n",
    "\n",
    "This simplified version should train successfully on Google Colab! üöÄ"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}