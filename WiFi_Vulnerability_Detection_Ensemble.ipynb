{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wifi-vuln-title"
   },
   "source": [
    "# Wi-Fi Vulnerability Detection System - Ensemble Model\n",
    "## AI Models Training & Deployment Guide\n",
    "\n",
    "**Project**: Wi-Fi Vulnerability Detection System  \n",
    "**Version**: 1.0  \n",
    "**Environment**: Google Colab Free  \n",
    "**Models**: CNN, LSTM, GNN, Crypto-BERT, Ensemble Fusion  \n",
    "\n",
    "This notebook implements the complete ensemble model architecture as specified in the project documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## 1. Environment Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow==2.13.0\n",
    "!pip install tensorflow-addons\n",
    "!pip install scikit-learn\n",
    "!pip install numpy pandas matplotlib seaborn\n",
    "!pip install networkx\n",
    "!pip install transformers\n",
    "!pip install imbalanced-learn\n",
    "!pip install plotly\n",
    "\n",
    "print(\"✅ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import-libraries"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import networkx as nx\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Memory limit set for GPU\" if tf.config.list_physical_devices('GPU') else \"Using CPU\")\n",
    "\n",
    "# Configure GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-generation"
   },
   "source": [
    "## 2. Synthetic Data Generation\n",
    "Since we don't have real network data, we'll generate realistic synthetic data based on the specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-generator-class"
   },
   "outputs": [],
   "source": [
    "class WiFiDataGenerator:\n",
    "    \"\"\"Generate synthetic Wi-Fi vulnerability data according to project specifications\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Class definitions from PDF\n",
    "        self.cnn_classes = [\n",
    "            'SECURE_NETWORK', 'WEAK_ENCRYPTION', 'OPEN_NETWORK', 'WPS_VULNERABILITY',\n",
    "            'ROGUE_AP', 'EVIL_TWIN', 'DEAUTH_ATTACK', 'HANDSHAKE_CAPTURE',\n",
    "            'FIRMWARE_OUTDATED', 'DEFAULT_CREDENTIALS', 'SIGNAL_LEAKAGE', 'UNKNOWN_THREAT'\n",
    "        ]\n",
    "        \n",
    "        self.lstm_classes = [\n",
    "            'NORMAL_BEHAVIOR', 'BRUTE_FORCE_ATTACK', 'RECONNAISSANCE', 'DATA_EXFILTRATION',\n",
    "            'BOTNET_ACTIVITY', 'INSIDER_THREAT', 'APT_BEHAVIOR', 'DDOS_PREPARATION',\n",
    "            'LATERAL_MOVEMENT', 'COMMAND_CONTROL'\n",
    "        ]\n",
    "        \n",
    "        self.gnn_classes = [\n",
    "            'ISOLATED_VULNERABILITY', 'CASCADING_RISK', 'CRITICAL_NODE', 'BRIDGE_VULNERABILITY',\n",
    "            'CLUSTER_WEAKNESS', 'PERIMETER_BREACH', 'PRIVILEGE_ESCALATION', 'NETWORK_PARTITION'\n",
    "        ]\n",
    "        \n",
    "        self.bert_classes = [\n",
    "            'STRONG_ENCRYPTION', 'WEAK_CIPHER_SUITE', 'CERTIFICATE_INVALID', 'KEY_REUSE',\n",
    "            'DOWNGRADE_ATTACK', 'MAN_IN_MIDDLE', 'REPLAY_ATTACK', 'TIMING_ATTACK',\n",
    "            'QUANTUM_VULNERABLE', 'ENTROPY_WEAKNESS', 'HASH_COLLISION', 'PADDING_ORACLE',\n",
    "            'LENGTH_EXTENSION', 'PROTOCOL_CONFUSION', 'CRYPTO_AGILITY_LACK'\n",
    "        ]\n",
    "        \n",
    "        self.ensemble_classes = [\n",
    "            'NO_THREAT', 'LOW_RISK_VULNERABILITY', 'MEDIUM_RISK_VULNERABILITY', 'HIGH_RISK_VULNERABILITY',\n",
    "            'CRITICAL_VULNERABILITY', 'ACTIVE_ATTACK_DETECTED', 'RECONNAISSANCE_PHASE', 'CREDENTIAL_COMPROMISE',\n",
    "            'DATA_BREACH_RISK', 'NETWORK_COMPROMISE', 'INSIDER_THREAT_DETECTED', 'APT_CAMPAIGN',\n",
    "            'RANSOMWARE_INDICATORS', 'BOTNET_PARTICIPATION', 'CRYPTO_WEAKNESS', 'FIRMWARE_EXPLOIT',\n",
    "            'CONFIGURATION_ERROR', 'COMPLIANCE_VIOLATION', 'ANOMALOUS_BEHAVIOR', 'SYSTEM_COMPROMISE'\n",
    "        ]\n",
    "    \n",
    "    def generate_cnn_data(self, n_samples=10000):\n",
    "        \"\"\"Generate CNN input data (32 dimensions) and labels (12 classes)\"\"\"\n",
    "        X = np.random.randn(n_samples, 32)\n",
    "        \n",
    "        # Add realistic patterns\n",
    "        # Signal strength metrics (0-7)\n",
    "        X[:, 0:8] = np.random.uniform(-90, -30, (n_samples, 8))  # RSSI, SNR values\n",
    "        \n",
    "        # Packet header features (8-15)\n",
    "        X[:, 8:16] = np.random.exponential(2, (n_samples, 8))\n",
    "        \n",
    "        # Encryption indicators (16-23)\n",
    "        X[:, 16:24] = np.random.binomial(1, 0.7, (n_samples, 8))\n",
    "        \n",
    "        # Traffic patterns (24-31)\n",
    "        X[:, 24:32] = np.random.lognormal(0, 1, (n_samples, 8))\n",
    "        \n",
    "        # Generate balanced labels\n",
    "        y = np.random.randint(0, len(self.cnn_classes), n_samples)\n",
    "        \n",
    "        return X.astype(np.float32), y\n",
    "    \n",
    "    def generate_lstm_data(self, n_samples=10000, sequence_length=50):\n",
    "        \"\"\"Generate LSTM sequence data (48 time-series dimensions) and labels (10 classes)\"\"\"\n",
    "        X = np.random.randn(n_samples, sequence_length, 48)\n",
    "        \n",
    "        # Add temporal patterns\n",
    "        for i in range(n_samples):\n",
    "            # Connection patterns (0-11)\n",
    "            X[i, :, 0:12] = np.cumsum(np.random.randn(sequence_length, 12) * 0.1, axis=0)\n",
    "            \n",
    "            # Data transfer rates (12-23)\n",
    "            X[i, :, 12:24] = np.abs(np.random.randn(sequence_length, 12)) * 100\n",
    "            \n",
    "            # Auth failures (24-35)\n",
    "            X[i, :, 24:36] = np.random.poisson(2, (sequence_length, 12))\n",
    "            \n",
    "            # Device behaviors (36-47)\n",
    "            X[i, :, 36:48] = np.random.exponential(1, (sequence_length, 12))\n",
    "        \n",
    "        y = np.random.randint(0, len(self.lstm_classes), n_samples)\n",
    "        return X.astype(np.float32), y\n",
    "    \n",
    "    def generate_gnn_data(self, n_samples=5000, n_nodes=20):\n",
    "        \"\"\"Generate GNN graph data with node/edge features\"\"\"\n",
    "        graphs = []\n",
    "        labels = []\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            # Create random graph\n",
    "            G = nx.erdos_renyi_graph(n_nodes, 0.3)\n",
    "            \n",
    "            # Node features (24 dimensions)\n",
    "            node_features = np.random.randn(n_nodes, 24)\n",
    "            # Device type (0-5)\n",
    "            node_features[:, 0:6] = np.random.randint(0, 5, (n_nodes, 6))\n",
    "            # Security config (6-11)\n",
    "            node_features[:, 6:12] = np.random.binomial(1, 0.6, (n_nodes, 6))\n",
    "            # Trust metrics (12-17)\n",
    "            node_features[:, 12:18] = np.random.uniform(0, 1, (n_nodes, 6))\n",
    "            # Vulnerability history (18-23)\n",
    "            node_features[:, 18:24] = np.random.exponential(1, (n_nodes, 6))\n",
    "            \n",
    "            # Edge features (16 dimensions)\n",
    "            edge_features = []\n",
    "            edges = list(G.edges())\n",
    "            \n",
    "            for _ in edges:\n",
    "                edge_feat = np.random.randn(16)\n",
    "                # Connection strength (0-3)\n",
    "                edge_feat[0:4] = np.random.uniform(0.1, 1.0, 4)\n",
    "                # Communication frequency (4-7)\n",
    "                edge_feat[4:8] = np.random.poisson(5, 4)\n",
    "                # Data flow (8-11)\n",
    "                edge_feat[8:12] = np.random.exponential(2, 4)\n",
    "                # Protocol compatibility (12-15)\n",
    "                edge_feat[12:16] = np.random.binomial(1, 0.8, 4)\n",
    "                edge_features.append(edge_feat)\n",
    "            \n",
    "            # For simplicity, convert to adjacency matrix representation\n",
    "            adj_matrix = nx.adjacency_matrix(G).toarray()\n",
    "            \n",
    "            graph_data = {\n",
    "                'adj_matrix': adj_matrix,\n",
    "                'node_features': node_features,\n",
    "                'edge_features': np.array(edge_features) if edge_features else np.zeros((0, 16))\n",
    "            }\n",
    "            \n",
    "            graphs.append(graph_data)\n",
    "            labels.append(np.random.randint(0, len(self.gnn_classes)))\n",
    "        \n",
    "        return graphs, np.array(labels)\n",
    "    \n",
    "    def generate_bert_data(self, n_samples=8000, sequence_length=512):\n",
    "        \"\"\"Generate BERT-style tokenized protocol sequences\"\"\"\n",
    "        # Simulate tokenized protocol sequences\n",
    "        vocab_size = 30000\n",
    "        X = np.random.randint(1, vocab_size, (n_samples, sequence_length))\n",
    "        \n",
    "        # Add special tokens\n",
    "        X[:, 0] = 1  # [CLS] token\n",
    "        \n",
    "        # Add realistic protocol patterns\n",
    "        for i in range(n_samples):\n",
    "            # Simulate handshake sequences\n",
    "            if i % 4 == 0:\n",
    "                X[i, 1:10] = np.arange(100, 110)  # Protocol start sequence\n",
    "            # Simulate certificate chains\n",
    "            elif i % 4 == 1:\n",
    "                X[i, 1:15] = np.arange(200, 215)  # Certificate sequence\n",
    "        \n",
    "        y = np.random.randint(0, len(self.bert_classes), n_samples)\n",
    "        return X, y\n",
    "    \n",
    "    def generate_all_data(self, n_samples=10000):\n",
    "        \"\"\"Generate all datasets for ensemble training\"\"\"\n",
    "        print(\"Generating synthetic Wi-Fi vulnerability datasets...\")\n",
    "        \n",
    "        # Generate data for each model\n",
    "        cnn_X, cnn_y = self.generate_cnn_data(n_samples)\n",
    "        lstm_X, lstm_y = self.generate_lstm_data(n_samples)\n",
    "        gnn_graphs, gnn_y = self.generate_gnn_data(n_samples//2)\n",
    "        bert_X, bert_y = self.generate_bert_data(n_samples)\n",
    "        \n",
    "        # Generate ensemble labels (more complex)\n",
    "        ensemble_y = np.random.randint(0, len(self.ensemble_classes), n_samples)\n",
    "        \n",
    "        print(f\"✅ Generated datasets:\")\n",
    "        print(f\"  CNN: {cnn_X.shape} -> {len(self.cnn_classes)} classes\")\n",
    "        print(f\"  LSTM: {lstm_X.shape} -> {len(self.lstm_classes)} classes\")\n",
    "        print(f\"  GNN: {len(gnn_graphs)} graphs -> {len(self.gnn_classes)} classes\")\n",
    "        print(f\"  BERT: {bert_X.shape} -> {len(self.bert_classes)} classes\")\n",
    "        print(f\"  Ensemble: {len(ensemble_y)} samples -> {len(self.ensemble_classes)} classes\")\n",
    "        \n",
    "        return {\n",
    "            'cnn': (cnn_X, cnn_y),\n",
    "            'lstm': (lstm_X, lstm_y),\n",
    "            'gnn': (gnn_graphs, gnn_y),\n",
    "            'bert': (bert_X, bert_y),\n",
    "            'ensemble_labels': ensemble_y\n",
    "        }\n",
    "\n",
    "# Generate data\n",
    "data_generator = WiFiDataGenerator()\n",
    "datasets = data_generator.generate_all_data(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-architectures"
   },
   "source": [
    "## 3. Model Architectures\n",
    "Implementing all five models according to PDF specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnn-model"
   },
   "outputs": [],
   "source": [
    "def build_cnn_model(input_dim=32, num_classes=12):\n",
    "    \"\"\"Build CNN model for pattern recognition in network traffic\n",
    "    Architecture from PDF: Deep Convolutional Network\n",
    "    Expected size: 45-60 MB, ~2.3M parameters\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Reshape for 1D convolution\n",
    "        layers.Reshape((input_dim, 1), input_shape=(input_dim,)),\n",
    "        \n",
    "        # Convolutional layers as specified\n",
    "        layers.Conv1D(64, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Conv1D(128, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Conv1D(256, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        layers.Conv1D(512, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GlobalMaxPooling1D(),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Dense layers with specified progression\n",
    "        layers.Dense(1024, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and display CNN model\n",
    "cnn_model = build_cnn_model(32, len(data_generator.cnn_classes))\n",
    "print(\"CNN Model Architecture:\")\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lstm-model"
   },
   "outputs": [],
   "source": [
    "def build_lstm_model(sequence_length=50, feature_dim=48, num_classes=10):\n",
    "    \"\"\"Build LSTM model for temporal analysis\n",
    "    Architecture: Bidirectional LSTM with Attention\n",
    "    Expected size: 35-50 MB, ~1.8M parameters\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Bidirectional LSTM layers as specified\n",
    "        layers.Bidirectional(\n",
    "            layers.LSTM(256, return_sequences=True, dropout=0.3, recurrent_dropout=0.2),\n",
    "            input_shape=(sequence_length, feature_dim)\n",
    "        ),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Bidirectional(\n",
    "            layers.LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)\n",
    "        ),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Bidirectional(\n",
    "            layers.LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.2)\n",
    "        ),\n",
    "        \n",
    "        # Attention mechanism (simplified)\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Output layers\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and display LSTM model\n",
    "lstm_model = build_lstm_model(50, 48, len(data_generator.lstm_classes))\n",
    "print(\"\\nLSTM Model Architecture:\")\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnn-model"
   },
   "outputs": [],
   "source": [
    "def build_gnn_model(n_nodes=20, node_features=24, num_classes=8):\n",
    "    \"\"\"Build Graph Neural Network for topology analysis\n",
    "    Simplified GNN using dense layers for Colab compatibility\n",
    "    Expected size: 25-40 MB, ~1.2M parameters\n",
    "    \"\"\"\n",
    "    # Node features input\n",
    "    node_input = layers.Input(shape=(n_nodes, node_features), name='node_features')\n",
    "    \n",
    "    # Adjacency matrix input\n",
    "    adj_input = layers.Input(shape=(n_nodes, n_nodes), name='adjacency_matrix')\n",
    "    \n",
    "    # Graph convolution simulation using dense layers\n",
    "    # First graph conv layer\n",
    "    x = layers.Dense(128, activation='relu')(node_input)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Message passing simulation\n",
    "    # x = adj @ x (matrix multiplication to aggregate neighbor information)\n",
    "    x = tf.keras.backend.batch_dot(adj_input, x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Second graph conv layer\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)\n",
    "    \n",
    "    # Another message passing\n",
    "    x = tf.keras.backend.batch_dot(adj_input, x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Global pooling (readout)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Final classification layers\n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(64, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=[node_input, adj_input], outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and display GNN model\n",
    "gnn_model = build_gnn_model(20, 24, len(data_generator.gnn_classes))\n",
    "print(\"\\nGNN Model Architecture:\")\n",
    "gnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bert-model"
   },
   "outputs": [],
   "source": [
    "def build_crypto_bert_model(vocab_size=30000, max_length=512, num_classes=15):\n",
    "    \"\"\"Build Crypto-BERT model for protocol analysis\n",
    "    Simplified transformer architecture for Colab\n",
    "    Expected size: 85-120 MB, ~4.2M parameters\n",
    "    \"\"\"\n",
    "    # Input layers\n",
    "    input_ids = layers.Input(shape=(max_length,), name='input_ids')\n",
    "    \n",
    "    # Embedding layers\n",
    "    embedding = layers.Embedding(vocab_size, 768, input_length=max_length)(input_ids)\n",
    "    embedding = layers.Dropout(0.1)(embedding)\n",
    "    \n",
    "    # Simplified transformer blocks\n",
    "    for i in range(6):  # Reduced from 12 for Colab\n",
    "        # Multi-head attention (simplified)\n",
    "        attention = layers.MultiHeadAttention(\n",
    "            num_heads=12, key_dim=64, dropout=0.1\n",
    "        )(embedding, embedding)\n",
    "        attention = layers.Dropout(0.1)(attention)\n",
    "        attention = layers.LayerNormalization()(embedding + attention)\n",
    "        \n",
    "        # Feed forward\n",
    "        ff = layers.Dense(3072, activation='gelu')(attention)\n",
    "        ff = layers.Dropout(0.1)(ff)\n",
    "        ff = layers.Dense(768)(ff)\n",
    "        ff = layers.Dropout(0.1)(ff)\n",
    "        embedding = layers.LayerNormalization()(attention + ff)\n",
    "    \n",
    "    # Pooling and classification\n",
    "    pooled = layers.GlobalAveragePooling1D()(embedding)\n",
    "    pooled = layers.Dense(768, activation='tanh')(pooled)\n",
    "    pooled = layers.Dropout(0.3)(pooled)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(pooled)\n",
    "    \n",
    "    model = models.Model(inputs=input_ids, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=2e-5),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and display BERT model (smaller version for Colab)\n",
    "bert_model = build_crypto_bert_model(30000, 256, len(data_generator.bert_classes))  # Reduced max_length\n",
    "print(\"\\nCrypto-BERT Model Architecture:\")\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ensemble-model"
   },
   "outputs": [],
   "source": [
    "def build_ensemble_model(cnn_classes=12, lstm_classes=10, gnn_classes=8, bert_classes=15, ensemble_classes=20):\n",
    "    \"\"\"Build Ensemble Fusion Model for meta-learning\n",
    "    Expected size: 15-25 MB, ~0.8M parameters\n",
    "    \"\"\"\n",
    "    # Input layers for each model's predictions\n",
    "    cnn_pred_input = layers.Input(shape=(cnn_classes,), name='cnn_predictions')\n",
    "    lstm_pred_input = layers.Input(shape=(lstm_classes,), name='lstm_predictions')\n",
    "    gnn_pred_input = layers.Input(shape=(gnn_classes,), name='gnn_predictions')\n",
    "    bert_pred_input = layers.Input(shape=(bert_classes,), name='bert_predictions')\n",
    "    \n",
    "    # Confidence scores input\n",
    "    confidence_input = layers.Input(shape=(4,), name='confidence_scores')\n",
    "    \n",
    "    # Concatenate all inputs\n",
    "    combined = layers.Concatenate()([\n",
    "        cnn_pred_input, lstm_pred_input, gnn_pred_input, bert_pred_input, confidence_input\n",
    "    ])\n",
    "    \n",
    "    # Fusion layers as specified in PDF\n",
    "    x = layers.Dense(256, activation='relu', kernel_regularizer=l2(0.001))(combined)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Confidence layers\n",
    "    confidence_branch = layers.Dense(32, activation='relu')(confidence_input)\n",
    "    confidence_branch = layers.Dense(16, activation='relu')(confidence_branch)\n",
    "    \n",
    "    # Severity layers\n",
    "    severity_branch = layers.Dense(64, activation='relu')(x)\n",
    "    severity_branch = layers.Dense(32, activation='relu')(severity_branch)\n",
    "    severity_branch = layers.Dense(16, activation='relu')(severity_branch)\n",
    "    \n",
    "    # Combine branches\n",
    "    final = layers.Concatenate()([x, confidence_branch, severity_branch])\n",
    "    final = layers.Dense(64, activation='relu')(final)\n",
    "    final = layers.Dropout(0.2)(final)\n",
    "    \n",
    "    outputs = layers.Dense(ensemble_classes, activation='softmax')(final)\n",
    "    \n",
    "    model = models.Model(\n",
    "        inputs=[cnn_pred_input, lstm_pred_input, gnn_pred_input, bert_pred_input, confidence_input],\n",
    "        outputs=outputs\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build ensemble model\n",
    "ensemble_model = build_ensemble_model(\n",
    "    len(data_generator.cnn_classes),\n",
    "    len(data_generator.lstm_classes), \n",
    "    len(data_generator.gnn_classes),\n",
    "    len(data_generator.bert_classes),\n",
    "    len(data_generator.ensemble_classes)\n",
    ")\n",
    "print(\"\\nEnsemble Model Architecture:\")\n",
    "ensemble_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-section"
   },
   "source": [
    "## 4. Training Setup & Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-preprocessing"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(datasets):\n",
    "    \"\"\"Preprocess all datasets according to PDF specifications\"\"\"\n",
    "    print(\"Preprocessing datasets...\")\n",
    "    \n",
    "    processed = {}\n",
    "    \n",
    "    # CNN data preprocessing\n",
    "    cnn_X, cnn_y = datasets['cnn']\n",
    "    scaler_cnn = StandardScaler()\n",
    "    cnn_X_scaled = scaler_cnn.fit_transform(cnn_X)\n",
    "    \n",
    "    # Train-test split for CNN\n",
    "    cnn_X_train, cnn_X_test, cnn_y_train, cnn_y_test = train_test_split(\n",
    "        cnn_X_scaled, cnn_y, test_size=0.3, stratify=cnn_y, random_state=42\n",
    "    )\n",
    "    \n",
    "    # LSTM data preprocessing\n",
    "    lstm_X, lstm_y = datasets['lstm']\n",
    "    scaler_lstm = StandardScaler()\n",
    "    lstm_X_scaled = scaler_lstm.fit_transform(lstm_X.reshape(-1, lstm_X.shape[-1])).reshape(lstm_X.shape)\n",
    "    \n",
    "    # Train-test split for LSTM\n",
    "    lstm_X_train, lstm_X_test, lstm_y_train, lstm_y_test = train_test_split(\n",
    "        lstm_X_scaled, lstm_y, test_size=0.3, stratify=lstm_y, random_state=42\n",
    "    )\n",
    "    \n",
    "    # GNN data preprocessing\n",
    "    gnn_graphs, gnn_y = datasets['gnn']\n",
    "    \n",
    "    # Convert graphs to numpy arrays for training\n",
    "    gnn_node_features = np.array([g['node_features'] for g in gnn_graphs])\n",
    "    gnn_adj_matrices = np.array([g['adj_matrix'] for g in gnn_graphs])\n",
    "    \n",
    "    # Normalize node features\n",
    "    scaler_gnn = StandardScaler()\n",
    "    gnn_node_features_scaled = scaler_gnn.fit_transform(\n",
    "        gnn_node_features.reshape(-1, gnn_node_features.shape[-1])\n",
    "    ).reshape(gnn_node_features.shape)\n",
    "    \n",
    "    # Train-test split for GNN\n",
    "    indices = np.arange(len(gnn_y))\n",
    "    train_idx, test_idx = train_test_split(indices, test_size=0.3, stratify=gnn_y, random_state=42)\n",
    "    \n",
    "    gnn_node_train, gnn_node_test = gnn_node_features_scaled[train_idx], gnn_node_features_scaled[test_idx]\n",
    "    gnn_adj_train, gnn_adj_test = gnn_adj_matrices[train_idx], gnn_adj_matrices[test_idx]\n",
    "    gnn_y_train, gnn_y_test = gnn_y[train_idx], gnn_y[test_idx]\n",
    "    \n",
    "    # BERT data preprocessing\n",
    "    bert_X, bert_y = datasets['bert']\n",
    "    \n",
    "    # Truncate sequences to fit Colab memory\n",
    "    bert_X = bert_X[:, :256]  # Reduced from 512\n",
    "    \n",
    "    # Train-test split for BERT\n",
    "    bert_X_train, bert_X_test, bert_y_train, bert_y_test = train_test_split(\n",
    "        bert_X, bert_y, test_size=0.3, stratify=bert_y, random_state=42\n",
    "    )\n",
    "    \n",
    "    processed = {\n",
    "        'cnn': {\n",
    "            'X_train': cnn_X_train, 'X_test': cnn_X_test,\n",
    "            'y_train': cnn_y_train, 'y_test': cnn_y_test,\n",
    "            'scaler': scaler_cnn\n",
    "        },\n",
    "        'lstm': {\n",
    "            'X_train': lstm_X_train, 'X_test': lstm_X_test,\n",
    "            'y_train': lstm_y_train, 'y_test': lstm_y_test,\n",
    "            'scaler': scaler_lstm\n",
    "        },\n",
    "        'gnn': {\n",
    "            'node_train': gnn_node_train, 'node_test': gnn_node_test,\n",
    "            'adj_train': gnn_adj_train, 'adj_test': gnn_adj_test,\n",
    "            'y_train': gnn_y_train, 'y_test': gnn_y_test,\n",
    "            'scaler': scaler_gnn\n",
    "        },\n",
    "        'bert': {\n",
    "            'X_train': bert_X_train, 'X_test': bert_X_test,\n",
    "            'y_train': bert_y_train, 'y_test': bert_y_test\n",
    "        },\n",
    "        'ensemble_labels': datasets['ensemble_labels']\n",
    "    }\n",
    "    \n",
    "    print(\"✅ Data preprocessing completed!\")\n",
    "    return processed\n",
    "\n",
    "# Preprocess all datasets\n",
    "processed_data = preprocess_data(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-callbacks"
   },
   "outputs": [],
   "source": [
    "def create_callbacks(model_name):\n",
    "    \"\"\"Create training callbacks as specified in PDF\"\"\"\n",
    "    callbacks_list = [\n",
    "        # Early stopping with patience=10\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Model checkpointing\n",
    "        callbacks.ModelCheckpoint(\n",
    "            f'{model_name}_best.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return callbacks_list\n",
    "\n",
    "# Training configuration\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64  # Optimized for Colab\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Early stopping patience: 10\")\n",
    "print(f\"  Learning rate scheduling: ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-training"
   },
   "source": [
    "## 5. Individual Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-cnn"
   },
   "outputs": [],
   "source": [
    "# Train CNN Model\n",
    "print(\"🔄 Training CNN Model...\")\n",
    "print(\"Target: Pattern recognition in network traffic\")\n",
    "print(\"Expected accuracy: 94-97%\")\n",
    "\n",
    "cnn_callbacks = create_callbacks('cnn_model')\n",
    "\n",
    "cnn_history = cnn_model.fit(\n",
    "    processed_data['cnn']['X_train'],\n",
    "    processed_data['cnn']['y_train'],\n",
    "    validation_data=(processed_data['cnn']['X_test'], processed_data['cnn']['y_test']),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=cnn_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate CNN\n",
    "cnn_loss, cnn_accuracy = cnn_model.evaluate(\n",
    "    processed_data['cnn']['X_test'], \n",
    "    processed_data['cnn']['y_test'], \n",
    "    verbose=0\n",
    ")\n",
    "print(f\"✅ CNN Model - Test Accuracy: {cnn_accuracy:.4f}\")\n",
    "\n",
    "# Save CNN model\n",
    "cnn_model.save('wifi_cnn_model.h5')\n",
    "print(\"💾 CNN model saved as 'wifi_cnn_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-lstm"
   },
   "outputs": [],
   "source": [
    "# Train LSTM Model\n",
    "print(\"\\n🔄 Training LSTM Model...\")\n",
    "print(\"Target: Temporal analysis of network behavior\")\n",
    "print(\"Expected accuracy: 91-94%\")\n",
    "\n",
    "lstm_callbacks = create_callbacks('lstm_model')\n",
    "\n",
    "lstm_history = lstm_model.fit(\n",
    "    processed_data['lstm']['X_train'],\n",
    "    processed_data['lstm']['y_train'],\n",
    "    validation_data=(processed_data['lstm']['X_test'], processed_data['lstm']['y_test']),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=lstm_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate LSTM\n",
    "lstm_loss, lstm_accuracy = lstm_model.evaluate(\n",
    "    processed_data['lstm']['X_test'], \n",
    "    processed_data['lstm']['y_test'], \n",
    "    verbose=0\n",
    ")\n",
    "print(f\"✅ LSTM Model - Test Accuracy: {lstm_accuracy:.4f}\")\n",
    "\n",
    "# Save LSTM model\n",
    "lstm_model.save('wifi_lstm_model.h5')\n",
    "print(\"💾 LSTM model saved as 'wifi_lstm_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-gnn"
   },
   "outputs": [],
   "source": [
    "# Train GNN Model\n",
    "print(\"\\n🔄 Training GNN Model...\")\n",
    "print(\"Target: Network topology analysis\")\n",
    "print(\"Expected accuracy: 88-92%\")\n",
    "\n",
    "gnn_callbacks = create_callbacks('gnn_model')\n",
    "\n",
    "gnn_history = gnn_model.fit(\n",
    "    [processed_data['gnn']['node_train'], processed_data['gnn']['adj_train']],\n",
    "    processed_data['gnn']['y_train'],\n",
    "    validation_data=(\n",
    "        [processed_data['gnn']['node_test'], processed_data['gnn']['adj_test']],\n",
    "        processed_data['gnn']['y_test']\n",
    "    ),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=gnn_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate GNN\n",
    "gnn_loss, gnn_accuracy = gnn_model.evaluate(\n",
    "    [processed_data['gnn']['node_test'], processed_data['gnn']['adj_test']], \n",
    "    processed_data['gnn']['y_test'], \n",
    "    verbose=0\n",
    ")\n",
    "print(f\"✅ GNN Model - Test Accuracy: {gnn_accuracy:.4f}\")\n",
    "\n",
    "# Save GNN model\n",
    "gnn_model.save('wifi_gnn_model.h5')\n",
    "print(\"💾 GNN model saved as 'wifi_gnn_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-bert"
   },
   "outputs": [],
   "source": [
    "# Train BERT Model (with reduced batch size for memory)\n",
    "print(\"\\n🔄 Training Crypto-BERT Model...\")\n",
    "print(\"Target: Protocol analysis and cryptographic detection\")\n",
    "print(\"Expected accuracy: 95-98%\")\n",
    "\n",
    "bert_callbacks = create_callbacks('bert_model')\n",
    "\n",
    "# Reduce batch size for BERT to fit in Colab memory\n",
    "bert_batch_size = 16\n",
    "\n",
    "bert_history = bert_model.fit(\n",
    "    processed_data['bert']['X_train'],\n",
    "    processed_data['bert']['y_train'],\n",
    "    validation_data=(processed_data['bert']['X_test'], processed_data['bert']['y_test']),\n",
    "    epochs=20,  # Reduced epochs for BERT\n",
    "    batch_size=bert_batch_size,\n",
    "    callbacks=bert_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate BERT\n",
    "bert_loss, bert_accuracy = bert_model.evaluate(\n",
    "    processed_data['bert']['X_test'], \n",
    "    processed_data['bert']['y_test'], \n",
    "    batch_size=bert_batch_size,\n",
    "    verbose=0\n",
    ")\n",
    "print(f\"✅ Crypto-BERT Model - Test Accuracy: {bert_accuracy:.4f}\")\n",
    "\n",
    "# Save BERT model\n",
    "bert_model.save('wifi_bert_model.h5')\n",
    "print(\"💾 Crypto-BERT model saved as 'wifi_bert_model.h5'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ensemble-training"
   },
   "source": [
    "## 6. Ensemble Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate-ensemble-data"
   },
   "outputs": [],
   "source": [
    "def generate_ensemble_training_data(processed_data, models):\n",
    "    \"\"\"Generate ensemble training data using predictions from individual models\"\"\"\n",
    "    print(\"Generating ensemble training data...\")\n",
    "    \n",
    "    cnn_model, lstm_model, gnn_model, bert_model = models\n",
    "    \n",
    "    # Get predictions from all models\n",
    "    cnn_pred = cnn_model.predict(processed_data['cnn']['X_train'], verbose=0)\n",
    "    lstm_pred = lstm_model.predict(processed_data['lstm']['X_train'], verbose=0)\n",
    "    gnn_pred = gnn_model.predict([\n",
    "        processed_data['gnn']['node_train'], \n",
    "        processed_data['gnn']['adj_train']\n",
    "    ], verbose=0)\n",
    "    bert_pred = bert_model.predict(processed_data['bert']['X_train'][:len(cnn_pred)], verbose=0)\n",
    "    \n",
    "    # Ensure all predictions have the same number of samples\n",
    "    min_samples = min(len(cnn_pred), len(lstm_pred), len(gnn_pred), len(bert_pred))\n",
    "    \n",
    "    cnn_pred = cnn_pred[:min_samples]\n",
    "    lstm_pred = lstm_pred[:min_samples]\n",
    "    gnn_pred = gnn_pred[:min_samples]\n",
    "    bert_pred = bert_pred[:min_samples]\n",
    "    \n",
    "    # Calculate confidence scores (max probability for each model)\n",
    "    confidence_scores = np.column_stack([\n",
    "        np.max(cnn_pred, axis=1),\n",
    "        np.max(lstm_pred, axis=1),\n",
    "        np.max(gnn_pred, axis=1),\n",
    "        np.max(bert_pred, axis=1)\n",
    "    ])\n",
    "    \n",
    "    # Use ensemble labels\n",
    "    ensemble_labels = processed_data['ensemble_labels'][:min_samples]\n",
    "    \n",
    "    return {\n",
    "        'cnn_pred': cnn_pred,\n",
    "        'lstm_pred': lstm_pred,\n",
    "        'gnn_pred': gnn_pred,\n",
    "        'bert_pred': bert_pred,\n",
    "        'confidence': confidence_scores,\n",
    "        'labels': ensemble_labels\n",
    "    }\n",
    "\n",
    "# Generate ensemble data\n",
    "ensemble_data = generate_ensemble_training_data(\n",
    "    processed_data, \n",
    "    [cnn_model, lstm_model, gnn_model, bert_model]\n",
    ")\n",
    "\n",
    "# Split ensemble data\n",
    "ensemble_inputs = [\n",
    "    ensemble_data['cnn_pred'],\n",
    "    ensemble_data['lstm_pred'],\n",
    "    ensemble_data['gnn_pred'],\n",
    "    ensemble_data['bert_pred'],\n",
    "    ensemble_data['confidence']\n",
    "]\n",
    "\n",
    "# Train-test split for ensemble\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
    "    [np.array(x) for x in ensemble_inputs],\n",
    "    ensemble_data['labels'],\n",
    "    test_size=0.3,\n",
    "    stratify=ensemble_data['labels'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Separate train and test inputs\n",
    "ensemble_train_inputs = [x for x in train_inputs]\n",
    "ensemble_test_inputs = [x for x in test_inputs]\n",
    "\n",
    "print(f\"✅ Ensemble data prepared: {len(train_labels)} training, {len(test_labels)} testing samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-ensemble"
   },
   "outputs": [],
   "source": [
    "# Train Ensemble Model\n",
    "print(\"🔄 Training Ensemble Fusion Model...\")\n",
    "print(\"Target: Meta-learning and decision fusion\")\n",
    "print(\"Expected accuracy: 96-99%\")\n",
    "\n",
    "ensemble_callbacks = create_callbacks('ensemble_model')\n",
    "\n",
    "ensemble_history = ensemble_model.fit(\n",
    "    ensemble_train_inputs,\n",
    "    train_labels,\n",
    "    validation_data=(ensemble_test_inputs, test_labels),\n",
    "    epochs=30,  # Fewer epochs for ensemble\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=ensemble_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate Ensemble\n",
    "ensemble_loss, ensemble_accuracy = ensemble_model.evaluate(\n",
    "    ensemble_test_inputs, \n",
    "    test_labels, \n",
    "    verbose=0\n",
    ")\n",
    "print(f\"✅ Ensemble Model - Test Accuracy: {ensemble_accuracy:.4f}\")\n",
    "\n",
    "# Save Ensemble model\n",
    "ensemble_model.save('wifi_ensemble_model.h5')\n",
    "print(\"💾 Ensemble model saved as 'wifi_ensemble_model.h5'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation-section"
   },
   "source": [
    "## 7. Model Evaluation & Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "performance-evaluation"
   },
   "outputs": [],
   "source": [
    "def evaluate_model_performance(models, test_data, class_names_dict):\n",
    "    \"\"\"Comprehensive evaluation of all models\"\"\"\n",
    "    print(\"📊 Model Performance Evaluation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # CNN Evaluation\n",
    "    cnn_pred = models['cnn'].predict(test_data['cnn']['X_test'], verbose=0)\n",
    "    cnn_pred_classes = np.argmax(cnn_pred, axis=1)\n",
    "    \n",
    "    print(f\"\\n🔸 CNN Model Performance:\")\n",
    "    print(f\"  Test Accuracy: {cnn_accuracy:.4f}\")\n",
    "    print(f\"  Target Range: 94-97%\")\n",
    "    print(f\"  Status: {'✅ Within Range' if 0.94 <= cnn_accuracy <= 0.97 else '⚠️ Outside Range'}\")\n",
    "    \n",
    "    # LSTM Evaluation\n",
    "    lstm_pred = models['lstm'].predict(test_data['lstm']['X_test'], verbose=0)\n",
    "    lstm_pred_classes = np.argmax(lstm_pred, axis=1)\n",
    "    \n",
    "    print(f\"\\n🔸 LSTM Model Performance:\")\n",
    "    print(f\"  Test Accuracy: {lstm_accuracy:.4f}\")\n",
    "    print(f\"  Target Range: 91-94%\")\n",
    "    print(f\"  Status: {'✅ Within Range' if 0.91 <= lstm_accuracy <= 0.94 else '⚠️ Outside Range'}\")\n",
    "    \n",
    "    # GNN Evaluation\n",
    "    gnn_pred = models['gnn'].predict([\n",
    "        test_data['gnn']['node_test'], \n",
    "        test_data['gnn']['adj_test']\n",
    "    ], verbose=0)\n",
    "    gnn_pred_classes = np.argmax(gnn_pred, axis=1)\n",
    "    \n",
    "    print(f\"\\n🔸 GNN Model Performance:\")\n",
    "    print(f\"  Test Accuracy: {gnn_accuracy:.4f}\")\n",
    "    print(f\"  Target Range: 88-92%\")\n",
    "    print(f\"  Status: {'✅ Within Range' if 0.88 <= gnn_accuracy <= 0.92 else '⚠️ Outside Range'}\")\n",
    "    \n",
    "    # BERT Evaluation\n",
    "    bert_pred = models['bert'].predict(test_data['bert']['X_test'], batch_size=16, verbose=0)\n",
    "    bert_pred_classes = np.argmax(bert_pred, axis=1)\n",
    "    \n",
    "    print(f\"\\n🔸 Crypto-BERT Model Performance:\")\n",
    "    print(f\"  Test Accuracy: {bert_accuracy:.4f}\")\n",
    "    print(f\"  Target Range: 95-98%\")\n",
    "    print(f\"  Status: {'✅ Within Range' if 0.95 <= bert_accuracy <= 0.98 else '⚠️ Outside Range'}\")\n",
    "    \n",
    "    # Ensemble Evaluation\n",
    "    print(f\"\\n🔸 Ensemble Model Performance:\")\n",
    "    print(f\"  Test Accuracy: {ensemble_accuracy:.4f}\")\n",
    "    print(f\"  Target Range: 96-99%\")\n",
    "    print(f\"  Status: {'✅ Within Range' if 0.96 <= ensemble_accuracy <= 0.99 else '⚠️ Outside Range'}\")\n",
    "    \n",
    "    return {\n",
    "        'cnn_accuracy': cnn_accuracy,\n",
    "        'lstm_accuracy': lstm_accuracy,\n",
    "        'gnn_accuracy': gnn_accuracy,\n",
    "        'bert_accuracy': bert_accuracy,\n",
    "        'ensemble_accuracy': ensemble_accuracy\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "model_dict = {\n",
    "    'cnn': cnn_model,\n",
    "    'lstm': lstm_model,\n",
    "    'gnn': gnn_model,\n",
    "    'bert': bert_model,\n",
    "    'ensemble': ensemble_model\n",
    "}\n",
    "\n",
    "class_names = {\n",
    "    'cnn': data_generator.cnn_classes,\n",
    "    'lstm': data_generator.lstm_classes,\n",
    "    'gnn': data_generator.gnn_classes,\n",
    "    'bert': data_generator.bert_classes,\n",
    "    'ensemble': data_generator.ensemble_classes\n",
    "}\n",
    "\n",
    "performance_results = evaluate_model_performance(model_dict, processed_data, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualization"
   },
   "outputs": [],
   "source": [
    "# Visualize Training History\n",
    "def plot_training_history(histories, model_names):\n",
    "    \"\"\"Plot training history for all models\"\"\"\n",
    "    fig, axes = plt.subplots(2, len(histories), figsize=(20, 8))\n",
    "    \n",
    "    for i, (history, name) in enumerate(zip(histories, model_names)):\n",
    "        # Accuracy plot\n",
    "        axes[0, i].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        axes[0, i].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "        axes[0, i].set_title(f'{name} - Accuracy')\n",
    "        axes[0, i].set_xlabel('Epochs')\n",
    "        axes[0, i].set_ylabel('Accuracy')\n",
    "        axes[0, i].legend()\n",
    "        axes[0, i].grid(True)\n",
    "        \n",
    "        # Loss plot\n",
    "        axes[1, i].plot(history.history['loss'], label='Train Loss')\n",
    "        axes[1, i].plot(history.history['val_loss'], label='Val Loss')\n",
    "        axes[1, i].set_title(f'{name} - Loss')\n",
    "        axes[1, i].set_xlabel('Epochs')\n",
    "        axes[1, i].set_ylabel('Loss')\n",
    "        axes[1, i].legend()\n",
    "        axes[1, i].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training histories\n",
    "histories = [cnn_history, lstm_history, gnn_history, bert_history, ensemble_history]\n",
    "model_names = ['CNN', 'LSTM', 'GNN', 'BERT', 'Ensemble']\n",
    "\n",
    "plot_training_history(histories, model_names)\n",
    "\n",
    "# Performance comparison chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "models = ['CNN', 'LSTM', 'GNN', 'BERT', 'Ensemble']\n",
    "accuracies = [cnn_accuracy, lstm_accuracy, gnn_accuracy, bert_accuracy, ensemble_accuracy]\n",
    "targets_min = [0.94, 0.91, 0.88, 0.95, 0.96]\n",
    "targets_max = [0.97, 0.94, 0.92, 0.98, 0.99]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "plt.bar(x, accuracies, alpha=0.8, color=['blue', 'green', 'orange', 'red', 'purple'])\n",
    "\n",
    "# Add target ranges\n",
    "for i, (model, acc, min_target, max_target) in enumerate(zip(models, accuracies, targets_min, targets_max)):\n",
    "    plt.errorbar(i, (min_target + max_target)/2, yerr=[(max_target - min_target)/2, (max_target - min_target)/2], \n",
    "                fmt='_', color='black', capsize=10, capthick=2, label='Target Range' if i == 0 else \"\")\n",
    "    plt.text(i, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Wi-Fi Vulnerability Detection System - Model Performance')\n",
    "plt.xticks(x, models)\n",
    "plt.ylim(0.8, 1.0)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-serving"
   },
   "source": [
    "## 8. Model Serving & Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference-pipeline"
   },
   "outputs": [],
   "source": [
    "class WiFiVulnerabilityDetector:\n",
    "    \"\"\"Complete Wi-Fi Vulnerability Detection System\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.class_names = {\n",
    "            'cnn': data_generator.cnn_classes,\n",
    "            'lstm': data_generator.lstm_classes,\n",
    "            'gnn': data_generator.gnn_classes,\n",
    "            'bert': data_generator.bert_classes,\n",
    "            'ensemble': data_generator.ensemble_classes\n",
    "        }\n",
    "        \n",
    "    def load_models(self, model_paths=None):\n",
    "        \"\"\"Load all trained models\"\"\"\n",
    "        if model_paths is None:\n",
    "            model_paths = {\n",
    "                'cnn': 'wifi_cnn_model.h5',\n",
    "                'lstm': 'wifi_lstm_model.h5',\n",
    "                'gnn': 'wifi_gnn_model.h5',\n",
    "                'bert': 'wifi_bert_model.h5',\n",
    "                'ensemble': 'wifi_ensemble_model.h5'\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            self.models['cnn'] = tf.keras.models.load_model(model_paths['cnn'])\n",
    "            self.models['lstm'] = tf.keras.models.load_model(model_paths['lstm'])\n",
    "            self.models['gnn'] = tf.keras.models.load_model(model_paths['gnn'])\n",
    "            self.models['bert'] = tf.keras.models.load_model(model_paths['bert'])\n",
    "            self.models['ensemble'] = tf.keras.models.load_model(model_paths['ensemble'])\n",
    "            \n",
    "            # Load scalers\n",
    "            self.scalers['cnn'] = processed_data['cnn']['scaler']\n",
    "            self.scalers['lstm'] = processed_data['lstm']['scaler']\n",
    "            self.scalers['gnn'] = processed_data['gnn']['scaler']\n",
    "            \n",
    "            print(\"✅ All models loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading models: {e}\")\n",
    "    \n",
    "    def predict_vulnerability(self, network_data):\n",
    "        \"\"\"Complete vulnerability prediction pipeline\"\"\"\n",
    "        # Extract features for each model\n",
    "        cnn_features = self._extract_cnn_features(network_data)\n",
    "        lstm_features = self._extract_lstm_features(network_data)\n",
    "        gnn_features = self._extract_gnn_features(network_data)\n",
    "        bert_features = self._extract_bert_features(network_data)\n",
    "        \n",
    "        # Get predictions from individual models\n",
    "        cnn_pred = self.models['cnn'].predict(cnn_features, verbose=0)\n",
    "        lstm_pred = self.models['lstm'].predict(lstm_features, verbose=0)\n",
    "        gnn_pred = self.models['gnn'].predict(gnn_features, verbose=0)\n",
    "        bert_pred = self.models['bert'].predict(bert_features, verbose=0)\n",
    "        \n",
    "        # Calculate confidence scores\n",
    "        confidence_scores = np.array([[  \n",
    "            np.max(cnn_pred),\n",
    "            np.max(lstm_pred), \n",
    "            np.max(gnn_pred),\n",
    "            np.max(bert_pred)\n",
    "        ]])\n",
    "        \n",
    "        # Ensemble prediction\n",
    "        ensemble_inputs = [cnn_pred, lstm_pred, gnn_pred, bert_pred, confidence_scores]\n",
    "        ensemble_pred = self.models['ensemble'].predict(ensemble_inputs, verbose=0)\n",
    "        \n",
    "        # Format results\n",
    "        results = {\n",
    "            'individual_predictions': {\n",
    "                'cnn': {\n",
    "                    'class': self.class_names['cnn'][np.argmax(cnn_pred)],\n",
    "                    'confidence': float(np.max(cnn_pred)),\n",
    "                    'probabilities': cnn_pred[0].tolist()\n",
    "                },\n",
    "                'lstm': {\n",
    "                    'class': self.class_names['lstm'][np.argmax(lstm_pred)],\n",
    "                    'confidence': float(np.max(lstm_pred)),\n",
    "                    'probabilities': lstm_pred[0].tolist()\n",
    "                },\n",
    "                'gnn': {\n",
    "                    'class': self.class_names['gnn'][np.argmax(gnn_pred)],\n",
    "                    'confidence': float(np.max(gnn_pred)),\n",
    "                    'probabilities': gnn_pred[0].tolist()\n",
    "                },\n",
    "                'bert': {\n",
    "                    'class': self.class_names['bert'][np.argmax(bert_pred)],\n",
    "                    'confidence': float(np.max(bert_pred)),\n",
    "                    'probabilities': bert_pred[0].tolist()\n",
    "                }\n",
    "            },\n",
    "            'ensemble_prediction': {\n",
    "                'final_class': self.class_names['ensemble'][np.argmax(ensemble_pred)],\n",
    "                'confidence': float(np.max(ensemble_pred)),\n",
    "                'risk_score': self._calculate_risk_score(ensemble_pred),\n",
    "                'probabilities': ensemble_pred[0].tolist()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _extract_cnn_features(self, network_data):\n",
    "        \"\"\"Extract CNN features from network data\"\"\"\n",
    "        # Simulate feature extraction\n",
    "        features = np.random.randn(1, 32).astype(np.float32)\n",
    "        return self.scalers['cnn'].transform(features)\n",
    "    \n",
    "    def _extract_lstm_features(self, network_data):\n",
    "        \"\"\"Extract LSTM features from network data\"\"\"\n",
    "        features = np.random.randn(1, 50, 48).astype(np.float32)\n",
    "        return features\n",
    "    \n",
    "    def _extract_gnn_features(self, network_data):\n",
    "        \"\"\"Extract GNN features from network data\"\"\"\n",
    "        node_features = np.random.randn(1, 20, 24).astype(np.float32)\n",
    "        adj_matrix = np.random.randint(0, 2, (1, 20, 20)).astype(np.float32)\n",
    "        return [node_features, adj_matrix]\n",
    "    \n",
    "    def _extract_bert_features(self, network_data):\n",
    "        \"\"\"Extract BERT features from network data\"\"\"\n",
    "        features = np.random.randint(1, 30000, (1, 256))\n",
    "        return features\n",
    "    \n",
    "    def _calculate_risk_score(self, ensemble_pred):\n",
    "        \"\"\"Calculate risk score based on ensemble prediction\"\"\"\n",
    "        # Define risk weights for each class\n",
    "        risk_weights = {\n",
    "            0: 0.0,   # NO_THREAT\n",
    "            1: 0.2,   # LOW_RISK_VULNERABILITY\n",
    "            2: 0.4,   # MEDIUM_RISK_VULNERABILITY\n",
    "            3: 0.7,   # HIGH_RISK_VULNERABILITY\n",
    "            4: 0.9,   # CRITICAL_VULNERABILITY\n",
    "            5: 0.95,  # ACTIVE_ATTACK_DETECTED\n",
    "        }\n",
    "        \n",
    "        predicted_class = np.argmax(ensemble_pred)\n",
    "        confidence = np.max(ensemble_pred)\n",
    "        \n",
    "        base_risk = risk_weights.get(predicted_class, 0.5)\n",
    "        risk_score = base_risk * confidence\n",
    "        \n",
    "        return float(risk_score)\n",
    "\n",
    "# Initialize the detector\n",
    "detector = WiFiVulnerabilityDetector()\n",
    "detector.load_models()\n",
    "\n",
    "print(\"🎯 Wi-Fi Vulnerability Detection System Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo-prediction"
   },
   "outputs": [],
   "source": [
    "# Demo prediction\n",
    "print(\"🧪 Running Demo Prediction...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simulate network data input\n",
    "demo_network_data = {\n",
    "    'ssid': 'Demo_Network',\n",
    "    'bssid': '00:11:22:33:44:55',\n",
    "    'signal_strength': -45,\n",
    "    'encryption': 'WPA2',\n",
    "    'channel': 6\n",
    "}\n",
    "\n",
    "# Get prediction\n",
    "prediction_result = detector.predict_vulnerability(demo_network_data)\n",
    "\n",
    "# Display results\n",
    "print(f\"📊 Prediction Results for {demo_network_data['ssid']}:\")\n",
    "print(f\"\\n🔸 Individual Model Predictions:\")\n",
    "for model_name, pred in prediction_result['individual_predictions'].items():\n",
    "    print(f\"  {model_name.upper()}: {pred['class']} (Confidence: {pred['confidence']:.3f})\")\n",
    "\n",
    "ensemble_result = prediction_result['ensemble_prediction']\n",
    "print(f\"\\n🎯 Final Ensemble Prediction:\")\n",
    "print(f\"  Classification: {ensemble_result['final_class']}\")\n",
    "print(f\"  Confidence: {ensemble_result['confidence']:.3f}\")\n",
    "print(f\"  Risk Score: {ensemble_result['risk_score']:.3f}\")\n",
    "\n",
    "# Risk level interpretation\n",
    "risk_score = ensemble_result['risk_score']\n",
    "if risk_score < 0.2:\n",
    "    risk_level = \"🟢 LOW RISK\"\n",
    "elif risk_score < 0.5:\n",
    "    risk_level = \"🟡 MEDIUM RISK\"\n",
    "elif risk_score < 0.8:\n",
    "    risk_level = \"🟠 HIGH RISK\"\n",
    "else:\n",
    "    risk_level = \"🔴 CRITICAL RISK\"\n",
    "\n",
    "print(f\"  Risk Level: {risk_level}\")\n",
    "\n",
    "print(\"\\n✅ Demo completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deployment-section"
   },
   "source": [
    "## 9. Model Export & Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model-export"
   },
   "outputs": [],
   "source": [
    "# Model export and documentation\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def export_model_documentation():\n",
    "    \"\"\"Export comprehensive model documentation\"\"\"\n",
    "    \n",
    "    model_info = {\n",
    "        \"project\": \"Wi-Fi Vulnerability Detection System\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"created_date\": datetime.now().isoformat(),\n",
    "        \"framework\": \"TensorFlow 2.13.0\",\n",
    "        \"environment\": \"Google Colab\",\n",
    "        \"models\": {\n",
    "            \"cnn\": {\n",
    "                \"purpose\": \"Pattern recognition in network traffic and signal analysis\",\n",
    "                \"architecture\": \"Deep Convolutional Network\",\n",
    "                \"input_shape\": [32],\n",
    "                \"output_classes\": len(data_generator.cnn_classes),\n",
    "                \"class_names\": data_generator.cnn_classes,\n",
    "                \"test_accuracy\": float(cnn_accuracy),\n",
    "                \"target_accuracy\": \"94-97%\",\n",
    "                \"file_size_mb\": \"45-60\",\n",
    "                \"parameters\": \"~2.3M\"\n",
    "            },\n",
    "            \"lstm\": {\n",
    "                \"purpose\": \"Temporal analysis of network behavior and attack sequence detection\",\n",
    "                \"architecture\": \"Bidirectional LSTM with Attention\",\n",
    "                \"input_shape\": [50, 48],\n",
    "                \"output_classes\": len(data_generator.lstm_classes),\n",
    "                \"class_names\": data_generator.lstm_classes,\n",
    "                \"test_accuracy\": float(lstm_accuracy),\n",
    "                \"target_accuracy\": \"91-94%\",\n",
    "                \"file_size_mb\": \"35-50\",\n",
    "                \"parameters\": \"~1.8M\"\n",
    "            },\n",
    "            \"gnn\": {\n",
    "                \"purpose\": \"Network topology analysis and vulnerability propagation modeling\",\n",
    "                \"architecture\": \"Graph Convolutional Network\",\n",
    "                \"input_shape\": \"Graph with 20 nodes, 24 node features\",\n",
    "                \"output_classes\": len(data_generator.gnn_classes),\n",
    "                \"class_names\": data_generator.gnn_classes,\n",
    "                \"test_accuracy\": float(gnn_accuracy),\n",
    "                \"target_accuracy\": \"88-92%\",\n",
    "                \"file_size_mb\": \"25-40\",\n",
    "                \"parameters\": \"~1.2M\"\n",
    "            },\n",
    "            \"bert\": {\n",
    "                \"purpose\": \"Protocol analysis and cryptographic vulnerability detection\",\n",
    "                \"architecture\": \"Transformer-based Language Model\",\n",
    "                \"input_shape\": [256],\n",
    "                \"vocab_size\": 30000,\n",
    "                \"output_classes\": len(data_generator.bert_classes),\n",
    "                \"class_names\": data_generator.bert_classes,\n",
    "                \"test_accuracy\": float(bert_accuracy),\n",
    "                \"target_accuracy\": \"95-98%\",\n",
    "                \"file_size_mb\": \"85-120\",\n",
    "                \"parameters\": \"~4.2M\"\n",
    "            },\n",
    "            \"ensemble\": {\n",
    "                \"purpose\": \"Meta-learning and decision fusion from all specialized models\",\n",
    "                \"architecture\": \"Multi-Input Fusion Network\",\n",
    "                \"inputs\": \"Predictions from CNN, LSTM, GNN, BERT + confidence scores\",\n",
    "                \"output_classes\": len(data_generator.ensemble_classes),\n",
    "                \"class_names\": data_generator.ensemble_classes,\n",
    "                \"test_accuracy\": float(ensemble_accuracy),\n",
    "                \"target_accuracy\": \"96-99%\",\n",
    "                \"file_size_mb\": \"15-25\",\n",
    "                \"parameters\": \"~0.8M\"\n",
    "            }\n",
    "        },\n",
    "        \"performance_summary\": {\n",
    "            \"total_parameters\": \"~10.3M\",\n",
    "            \"total_model_size_mb\": \"~220-285\",\n",
    "            \"inference_latency_target\": \"<100ms per sample\",\n",
    "            \"throughput_target\": \">1000 samples/second\",\n",
    "            \"memory_usage_target\": \"<2GB total\"\n",
    "        },\n",
    "        \"deployment_requirements\": {\n",
    "            \"python_version\": \"3.9+\",\n",
    "            \"tensorflow_version\": \"2.13+\",\n",
    "            \"minimum_ram\": \"8GB\",\n",
    "            \"recommended_ram\": \"16GB\",\n",
    "            \"gpu_support\": \"Optional but recommended\",\n",
    "            \"cpu_cores\": \"4+ cores recommended\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save documentation\n",
    "    with open('wifi_vulnerability_models_info.json', 'w') as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "    \n",
    "    return model_info\n",
    "\n",
    "# Export documentation\n",
    "model_documentation = export_model_documentation()\n",
    "\n",
    "print(\"📋 Model Documentation Exported:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Project: {model_documentation['project']}\")\n",
    "print(f\"Version: {model_documentation['version']}\")\n",
    "print(f\"Total Models: {len(model_documentation['models'])}\")\n",
    "print(f\"Documentation saved as: wifi_vulnerability_models_info.json\")\n",
    "\n",
    "# Display file sizes of saved models\n",
    "print(\"\\n💾 Saved Model Files:\")\n",
    "model_files = [\n",
    "    'wifi_cnn_model.h5',\n",
    "    'wifi_lstm_model.h5', \n",
    "    'wifi_gnn_model.h5',\n",
    "    'wifi_bert_model.h5',\n",
    "    'wifi_ensemble_model.h5'\n",
    "]\n",
    "\n",
    "total_size = 0\n",
    "for file in model_files:\n",
    "    if os.path.exists(file):\n",
    "        size_mb = os.path.getsize(file) / (1024 * 1024)\n",
    "        total_size += size_mb\n",
    "        print(f\"  {file}: {size_mb:.2f} MB\")\n",
    "\n",
    "print(f\"\\n📊 Total Model Size: {total_size:.2f} MB\")\n",
    "print(f\"Target Range: 220-285 MB\")\n",
    "print(f\"Status: {'✅ Within Range' if 220 <= total_size <= 285 else '⚠️ Outside Range'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-section"
   },
   "source": [
    "## 10. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-summary"
   },
   "outputs": [],
   "source": [
    "# Final project summary\n",
    "print(\"🎯 Wi-Fi Vulnerability Detection System - Training Complete!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n📊 FINAL PERFORMANCE SUMMARY:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"CNN Model (Traffic Analysis):        {cnn_accuracy:.1%} (Target: 94-97%)\")\n",
    "print(f\"LSTM Model (Behavioral Analysis):    {lstm_accuracy:.1%} (Target: 91-94%)\")\n",
    "print(f\"GNN Model (Topology Analysis):       {gnn_accuracy:.1%} (Target: 88-92%)\")\n",
    "print(f\"BERT Model (Protocol Analysis):      {bert_accuracy:.1%} (Target: 95-98%)\")\n",
    "print(f\"Ensemble Model (Final Decision):     {ensemble_accuracy:.1%} (Target: 96-99%)\")\n",
    "\n",
    "print(\"\\n🎯 KEY ACHIEVEMENTS:\")\n",
    "print(\"✅ Implemented complete 5-model ensemble architecture\")\n",
    "print(\"✅ All models trained successfully on Google Colab Free\")\n",
    "print(\"✅ Synthetic data generation pipeline created\")\n",
    "print(\"✅ Comprehensive evaluation and visualization\")\n",
    "print(\"✅ Production-ready inference pipeline\")\n",
    "print(\"✅ Complete model documentation exported\")\n",
    "\n",
    "print(\"\\n📁 GENERATED FILES:\")\n",
    "files = [\n",
    "    \"wifi_cnn_model.h5 - CNN model for traffic pattern analysis\",\n",
    "    \"wifi_lstm_model.h5 - LSTM model for temporal behavior analysis\", \n",
    "    \"wifi_gnn_model.h5 - GNN model for network topology analysis\",\n",
    "    \"wifi_bert_model.h5 - Crypto-BERT model for protocol analysis\",\n",
    "    \"wifi_ensemble_model.h5 - Ensemble fusion model\",\n",
    "    \"wifi_vulnerability_models_info.json - Complete documentation\"\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    print(f\"  📄 {file}\")\n",
    "\n",
    "print(\"\\n🚀 NEXT STEPS FOR DEPLOYMENT:\")\n",
    "print(\"1. 📊 Collect real Wi-Fi network data for retraining\")\n",
    "print(\"2. 🔧 Integrate with Flask web application\")\n",
    "print(\"3. 🛡️ Implement security safeguards and ethical controls\")\n",
    "print(\"4. 📈 Set up monitoring and continuous learning pipeline\")\n",
    "print(\"5. 🧪 Conduct penetration testing in controlled environment\")\n",
    "print(\"6. 📋 Create operational documentation and user training\")\n",
    "\n",
    "print(\"\\n⚠️  IMPORTANT NOTES:\")\n",
    "print(\"• This system is for DEFENSIVE security purposes only\")\n",
    "print(\"• Only use on networks you own or have explicit permission to test\")\n",
    "print(\"• Follow all applicable laws and ethical guidelines\")\n",
    "print(\"• Implement proper access controls and audit logging\")\n",
    "\n",
    "print(\"\\n🎉 Training session completed successfully!\")\n",
    "print(\"📧 Contact your security team to proceed with deployment planning.\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",\n  "colab": {\n   "gpuType": "T4",\n   "provenance": []\n  },\n  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.10"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 0\n}