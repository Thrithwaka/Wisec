{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crypto-bert-header"
   },
   "source": [
    "# üîê Crypto-BERT Model Training (Colab Optimized)\n",
    "## Wi-Fi Vulnerability Detection System - Protocol Analysis Module\n",
    "\n",
    "**Optimized for Google Colab Free Tier**\n",
    "- Reduced model size for memory efficiency\n",
    "- Faster training with smaller dataset\n",
    "- Mixed precision training for speed\n",
    "- Memory-efficient data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## üì¶ Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q tensorflow==2.13.0\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q matplotlib seaborn\n",
    "!pip install -q tqdm\n",
    "\n",
    "# Setup memory growth and mixed precision\n",
    "import tensorflow as tf\n",
    "\n",
    "# Enable memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"üöÄ GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Enable mixed precision for faster training\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "print(f\"üéØ Mixed precision policy: {policy.name}\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config-section"
   },
   "source": [
    "## ‚öôÔ∏è Optimized Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bert-config"
   },
   "outputs": [],
   "source": [
    "# Optimized BERT Configuration for Colab\n",
    "class CryptoBERTConfig:\n",
    "    def __init__(self):\n",
    "        # Reduced model architecture for Colab\n",
    "        self.vocab_size = 15000  # Reduced from 30000\n",
    "        self.hidden_size = 384   # Reduced from 768\n",
    "        self.max_sequence_length = 256  # Reduced from 512\n",
    "        self.num_transformer_layers = 6   # Reduced from 12\n",
    "        self.num_attention_heads = 6      # Reduced from 12\n",
    "        self.intermediate_size = 1536     # 4 * hidden_size\n",
    "        self.dropout_rate = 0.1\n",
    "        \n",
    "        # Training parameters optimized for Colab\n",
    "        self.num_classes = 15\n",
    "        self.batch_size = 32  # Increased batch size for efficiency\n",
    "        self.learning_rate = 3e-4  # Slightly higher for faster convergence\n",
    "        self.epochs = 5  # Reduced epochs\n",
    "        self.samples_per_class = 800  # Reduced dataset size\n",
    "        \n",
    "        # Class labels\n",
    "        self.class_labels = [\n",
    "            'STRONG_ENCRYPTION',\n",
    "            'WEAK_CIPHER_SUITE',\n",
    "            'CERTIFICATE_INVALID',\n",
    "            'KEY_REUSE',\n",
    "            'DOWNGRADE_ATTACK',\n",
    "            'MAN_IN_MIDDLE',\n",
    "            'REPLAY_ATTACK',\n",
    "            'TIMING_ATTACK',\n",
    "            'QUANTUM_VULNERABLE',\n",
    "            'ENTROPY_WEAKNESS',\n",
    "            'HASH_COLLISION',\n",
    "            'PADDING_ORACLE',\n",
    "            'LENGTH_EXTENSION',\n",
    "            'PROTOCOL_CONFUSION',\n",
    "            'CRYPTO_AGILITY_LACK'\n",
    "        ]\n",
    "\n",
    "config = CryptoBERTConfig()\n",
    "print(f\"üìã Optimized Configuration:\")\n",
    "print(f\"   Vocab Size: {config.vocab_size:,}\")\n",
    "print(f\"   Hidden Size: {config.hidden_size}\")\n",
    "print(f\"   Sequence Length: {config.max_sequence_length}\")\n",
    "print(f\"   Transformer Layers: {config.num_transformer_layers}\")\n",
    "print(f\"   Attention Heads: {config.num_attention_heads}\")\n",
    "print(f\"   Batch Size: {config.batch_size}\")\n",
    "print(f\"   Samples per Class: {config.samples_per_class}\")\n",
    "print(f\"   Total Samples: {config.samples_per_class * config.num_classes:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-generation"
   },
   "source": [
    "## üîÑ Optimized Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "protocol-generator"
   },
   "outputs": [],
   "source": [
    "class OptimizedProtocolGenerator:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        # Simplified protocol templates\n",
    "        self.templates = {\n",
    "            'STRONG_ENCRYPTION': [\n",
    "                \"TLS1.3 AES-256-GCM ECDHE-P256 secure\",\n",
    "                \"WPA3 ChaCha20-Poly1305 strong encryption\",\n",
    "                \"TLS1.3 AES-128-GCM secure handshake\"\n",
    "            ],\n",
    "            'WEAK_CIPHER_SUITE': [\n",
    "                \"TLS1.0 RC4 weak cipher detected\",\n",
    "                \"WPA DES encryption vulnerable\",\n",
    "                \"SSL3.0 MD5 deprecated hash\"\n",
    "            ],\n",
    "            'CERTIFICATE_INVALID': [\n",
    "                \"TLS cert expired self-signed invalid\",\n",
    "                \"SSL certificate chain broken untrusted\",\n",
    "                \"HTTPS cert hostname mismatch\"\n",
    "            ],\n",
    "            'KEY_REUSE': [\n",
    "                \"WPA2 nonce reuse detected session key\",\n",
    "                \"TLS key exchange reused RSA\",\n",
    "                \"EAP session key material reused\"\n",
    "            ],\n",
    "            'DOWNGRADE_ATTACK': [\n",
    "                \"TLS1.3 downgrade to TLS1.0 detected\",\n",
    "                \"WPA3 downgrade to WPA2 forced\",\n",
    "                \"HTTPS downgrade to HTTP\"\n",
    "            ],\n",
    "            'MAN_IN_MIDDLE': [\n",
    "                \"TLS cert fingerprint mismatch MITM\",\n",
    "                \"DNS spoofing certificate invalid\",\n",
    "                \"ARP spoofing SSL interception\"\n",
    "            ],\n",
    "            'REPLAY_ATTACK': [\n",
    "                \"EAP message replay timestamp invalid\",\n",
    "                \"TLS handshake replay detected\",\n",
    "                \"WPA handshake message replayed\"\n",
    "            ],\n",
    "            'TIMING_ATTACK': [\n",
    "                \"RSA decrypt timing variation high\",\n",
    "                \"AES padding oracle timing leak\",\n",
    "                \"HMAC verification timing attack\"\n",
    "            ],\n",
    "            'QUANTUM_VULNERABLE': [\n",
    "                \"RSA-2048 quantum vulnerable algorithm\",\n",
    "                \"ECDSA-P256 post quantum needed\",\n",
    "                \"DH key exchange quantum weak\"\n",
    "            ],\n",
    "            'ENTROPY_WEAKNESS': [\n",
    "                \"PRNG entropy low predictable random\",\n",
    "                \"Random generation weak seed\",\n",
    "                \"Nonce generation entropy insufficient\"\n",
    "            ],\n",
    "            'HASH_COLLISION': [\n",
    "                \"MD5 hash collision detected same\",\n",
    "                \"SHA1 collision vulnerability found\",\n",
    "                \"Hash function collision attack\"\n",
    "            ],\n",
    "            'PADDING_ORACLE': [\n",
    "                \"AES-CBC padding oracle error leak\",\n",
    "                \"PKCS padding oracle attack possible\",\n",
    "                \"RSA padding oracle vulnerability\"\n",
    "            ],\n",
    "            'LENGTH_EXTENSION': [\n",
    "                \"SHA1 HMAC length extension possible\",\n",
    "                \"Hash length extension attack detected\",\n",
    "                \"MAC length extension vulnerability\"\n",
    "            ],\n",
    "            'PROTOCOL_CONFUSION': [\n",
    "                \"TLS HTTP protocol confusion detected\",\n",
    "                \"SSL strip protocol downgrade\",\n",
    "                \"Mixed protocol implementation flaw\"\n",
    "            ],\n",
    "            'CRYPTO_AGILITY_LACK': [\n",
    "                \"Legacy system crypto agility limited\",\n",
    "                \"Single cipher suite support only\",\n",
    "                \"No algorithm upgrade path available\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Additional variations\n",
    "        self.variations = [\n",
    "            \"session {}\", \"timestamp {}\", \"id {}\", \"port {}\", \n",
    "            \"version {}\", \"length {}\", \"flags {}\", \"data {}\"\n",
    "        ]\n",
    "    \n",
    "    def generate_sequence(self, class_name):\n",
    "        \"\"\"Generate a single sequence for given class\"\"\"\n",
    "        base_template = random.choice(self.templates[class_name])\n",
    "        \n",
    "        # Add random variations\n",
    "        if random.random() < 0.3:\n",
    "            variation = random.choice(self.variations).format(random.randint(1000, 9999))\n",
    "            base_template += f\" {variation}\"\n",
    "        \n",
    "        return base_template\n",
    "    \n",
    "    def generate_dataset(self):\n",
    "        \"\"\"Generate complete dataset\"\"\"\n",
    "        sequences = []\n",
    "        labels = []\n",
    "        \n",
    "        print(\"üîÑ Generating optimized dataset...\")\n",
    "        \n",
    "        for class_idx, class_name in enumerate(tqdm(self.config.class_labels)):\n",
    "            for _ in range(self.config.samples_per_class):\n",
    "                sequence = self.generate_sequence(class_name)\n",
    "                sequences.append(sequence)\n",
    "                labels.append(class_idx)\n",
    "        \n",
    "        return sequences, labels\n",
    "\n",
    "# Generate optimized dataset\n",
    "generator = OptimizedProtocolGenerator(config)\n",
    "sequences, labels = generator.generate_dataset()\n",
    "\n",
    "print(f\"‚úÖ Generated {len(sequences):,} sequences\")\n",
    "print(f\"üìù Sample: {sequences[0]}\")\n",
    "\n",
    "# Clear memory\n",
    "del generator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tokenization"
   },
   "source": [
    "## üî§ Optimized Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tokenizer"
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab_size, max_length):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.word_to_id = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3}\n",
    "        self.current_id = 4\n",
    "    \n",
    "    def build_vocab(self, sequences):\n",
    "        \"\"\"Build vocabulary efficiently\"\"\"\n",
    "        print(\"üî§ Building vocabulary...\")\n",
    "        \n",
    "        # Count words\n",
    "        word_freq = {}\n",
    "        for seq in tqdm(sequences, desc=\"Counting words\"):\n",
    "            words = seq.lower().split()\n",
    "            for word in words:\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        \n",
    "        # Add frequent words\n",
    "        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for word, freq in sorted_words:\n",
    "            if self.current_id >= self.vocab_size:\n",
    "                break\n",
    "            self.word_to_id[word] = self.current_id\n",
    "            self.current_id += 1\n",
    "        \n",
    "        print(f\"‚úÖ Vocabulary built: {len(self.word_to_id)} tokens\")\n",
    "    \n",
    "    def encode_batch(self, sequences):\n",
    "        \"\"\"Encode sequences in batches for memory efficiency\"\"\"\n",
    "        encoded = []\n",
    "        \n",
    "        for seq in tqdm(sequences, desc=\"Encoding\"):\n",
    "            words = seq.lower().split()\n",
    "            \n",
    "            # Convert to IDs\n",
    "            ids = [self.word_to_id['[CLS]']]\n",
    "            for word in words[:self.max_length-2]:\n",
    "                ids.append(self.word_to_id.get(word, self.word_to_id['[UNK]']))\n",
    "            ids.append(self.word_to_id['[SEP]'])\n",
    "            \n",
    "            # Pad\n",
    "            while len(ids) < self.max_length:\n",
    "                ids.append(self.word_to_id['[PAD]'])\n",
    "            \n",
    "            encoded.append(ids[:self.max_length])\n",
    "        \n",
    "        return np.array(encoded, dtype=np.int32)\n",
    "\n",
    "# Build tokenizer\n",
    "tokenizer = SimpleTokenizer(config.vocab_size, config.max_sequence_length)\n",
    "tokenizer.build_vocab(sequences)\n",
    "\n",
    "# Encode sequences\n",
    "X = tokenizer.encode_batch(sequences)\n",
    "y = np.array(labels, dtype=np.int32)\n",
    "\n",
    "print(f\"üìù Encoded shape: {X.shape}\")\n",
    "print(f\"üè∑Ô∏è Labels shape: {y.shape}\")\n",
    "\n",
    "# Clear intermediate data\n",
    "del sequences, labels\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-split"
   },
   "source": [
    "## üìä Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "split-data"
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, stratify=y_train, random_state=SEED\n",
    ")\n",
    "\n",
    "# Convert to categorical\n",
    "y_train_cat = to_categorical(y_train, config.num_classes)\n",
    "y_val_cat = to_categorical(y_val, config.num_classes)\n",
    "y_test_cat = to_categorical(y_test, config.num_classes)\n",
    "\n",
    "print(f\"üìä Data splits:\")\n",
    "print(f\"   Train: {X_train.shape[0]:,}\")\n",
    "print(f\"   Val: {X_val.shape[0]:,}\")\n",
    "print(f\"   Test: {X_test.shape[0]:,}\")\n",
    "\n",
    "# Clear original data\n",
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-architecture"
   },
   "source": [
    "## üèóÔ∏è Lightweight BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bert-model"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"Efficient attention implementation\"\"\"\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    \n",
    "    # Scale\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    # Apply mask\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    \n",
    "    # Softmax\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    \n",
    "    return output\n",
    "\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = layers.Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "        q = self.wq(inputs)\n",
    "        k = self.wk(inputs)\n",
    "        v = self.wv(inputs)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        scaled_attention = scaled_dot_product_attention(q, k, v)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        \n",
    "        output = self.dense(concat_attention)\n",
    "        return output\n",
    "\n",
    "def create_lightweight_bert(config):\n",
    "    \"\"\"Create memory-efficient BERT model\"\"\"\n",
    "    \n",
    "    inputs = layers.Input(shape=(config.max_sequence_length,), dtype=tf.int32)\n",
    "    \n",
    "    # Embedding\n",
    "    embedding = layers.Embedding(\n",
    "        config.vocab_size, \n",
    "        config.hidden_size,\n",
    "        mask_zero=True\n",
    "    )(inputs)\n",
    "    \n",
    "    # Positional encoding\n",
    "    position_encoding = layers.Embedding(\n",
    "        config.max_sequence_length, \n",
    "        config.hidden_size\n",
    "    )(tf.range(config.max_sequence_length))\n",
    "    \n",
    "    x = embedding + position_encoding\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = layers.Dropout(config.dropout_rate)(x)\n",
    "    \n",
    "    # Transformer blocks\n",
    "    for i in range(config.num_transformer_layers):\n",
    "        # Multi-head attention\n",
    "        attn_output = MultiHeadAttention(config.hidden_size, config.num_attention_heads)(x)\n",
    "        attn_output = layers.Dropout(config.dropout_rate)(attn_output)\n",
    "        out1 = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "        \n",
    "        # Feed forward\n",
    "        ffn_output = layers.Dense(config.intermediate_size, activation='gelu')(out1)\n",
    "        ffn_output = layers.Dense(config.hidden_size)(ffn_output)\n",
    "        ffn_output = layers.Dropout(config.dropout_rate)(ffn_output)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "    \n",
    "    # Classification head\n",
    "    cls_token = x[:, 0, :]\n",
    "    cls_token = layers.Dropout(config.dropout_rate)(cls_token)\n",
    "    \n",
    "    # Output layers\n",
    "    dense = layers.Dense(256, activation='gelu')(cls_token)\n",
    "    dense = layers.Dropout(config.dropout_rate)(dense)\n",
    "    \n",
    "    # Final output with float32 for numerical stability\n",
    "    outputs = layers.Dense(config.num_classes, activation='softmax', dtype='float32')(dense)\n",
    "    \n",
    "    model = Model(inputs, outputs, name='LightweightCryptoBERT')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "print(\"üèóÔ∏è Creating lightweight BERT model...\")\n",
    "model = create_lightweight_bert(config)\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "total_params = model.count_params()\n",
    "model_size_mb = total_params * 4 / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nüìä Model Stats:\")\n",
    "print(f\"   Parameters: {total_params:,}\")\n",
    "print(f\"   Estimated Size: {model_size_mb:.1f} MB\")\n",
    "print(f\"   Memory per batch: ~{config.batch_size * config.max_sequence_length * 4 / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-setup"
   },
   "source": [
    "## üéØ Optimized Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-config"
   },
   "outputs": [],
   "source": [
    "# Optimized training configuration\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=config.learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999\n",
    ")\n",
    "\n",
    "# Compile with loss scaling for mixed precision\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Optimized callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=2,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=1,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"üéØ Training config:\")\n",
    "print(f\"   Learning Rate: {config.learning_rate}\")\n",
    "print(f\"   Batch Size: {config.batch_size}\")\n",
    "print(f\"   Epochs: {config.epochs}\")\n",
    "print(f\"   Mixed Precision: {tf.keras.mixed_precision.global_policy().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## üöÄ Fast Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-model"
   },
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting optimized training...\")\n",
    "print(f\"üìä Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"üîç Validation samples: {X_val.shape[0]:,}\")\n",
    "\n",
    "# Train with optimized settings\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    batch_size=config.batch_size,\n",
    "    epochs=config.epochs,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    "    workers=2\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training completed!\")\n",
    "\n",
    "# Clear training data from memory\n",
    "del X_train, y_train_cat, X_val, y_val_cat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## üìà Quick Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test evaluation\n",
    "print(\"üß™ Testing...\")\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_cat, batch_size=config.batch_size, verbose=0)\n",
    "\n",
    "print(f\"\\nüìä Final Results:\")\n",
    "print(f\"   Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "print(f\"   Best Val Accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# Quick predictions\n",
    "y_pred = model.predict(X_test, batch_size=config.batch_size, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "print(f\"\\nüéØ Classification Report (Top 5 classes):\")\n",
    "for i in range(5):\n",
    "    class_acc = np.mean(y_pred_classes[y_true_classes == i] == i) if np.sum(y_true_classes == i) > 0 else 0\n",
    "    print(f\"   {config.class_labels[i][:20]}: {class_acc:.3f}\")\n",
    "\n",
    "# Memory cleanup\n",
    "del X_test, y_test_cat, y_pred\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save-model"
   },
   "source": [
    "## üíæ Save Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-files"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = 'crypto_bert_optimized.h5'\n",
    "model.save(model_path)\n",
    "print(f\"üíæ Model saved: {model_path}\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer_data = {\n",
    "    'word_to_id': tokenizer.word_to_id,\n",
    "    'vocab_size': tokenizer.vocab_size,\n",
    "    'max_length': tokenizer.max_length\n",
    "}\n",
    "\n",
    "with open('tokenizer_optimized.json', 'w') as f:\n",
    "    json.dump(tokenizer_data, f)\n",
    "print(\"üíæ Tokenizer saved: tokenizer_optimized.json\")\n",
    "\n",
    "# Save config\n",
    "model_config = {\n",
    "    'vocab_size': config.vocab_size,\n",
    "    'hidden_size': config.hidden_size,\n",
    "    'max_sequence_length': config.max_sequence_length,\n",
    "    'num_transformer_layers': config.num_transformer_layers,\n",
    "    'num_attention_heads': config.num_attention_heads,\n",
    "    'num_classes': config.num_classes,\n",
    "    'class_labels': config.class_labels,\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'total_parameters': int(total_params),\n",
    "    'model_size_mb': float(model_size_mb)\n",
    "}\n",
    "\n",
    "with open('config_optimized.json', 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "print(\"üíæ Config saved: config_optimized.json\")\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "actual_size = os.path.getsize(model_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nüìè Final Model Stats:\")\n",
    "print(f\"   File Size: {actual_size:.1f} MB\")\n",
    "print(f\"   Parameters: {total_params:,}\")\n",
    "print(f\"   Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"   Training Time: Much faster! ‚ö°\")\n",
    "\n",
    "print(f\"\\nüéâ Optimized Crypto-BERT Ready!\")\n",
    "print(f\"   ‚úÖ Smaller model size\")\n",
    "print(f\"   ‚úÖ Faster training\")\n",
    "print(f\"   ‚úÖ Memory efficient\")\n",
    "print(f\"   ‚úÖ Colab compatible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## üì• Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-files"
   },
   "outputs": [],
   "source": [
    "# Download in Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"üì• Downloading optimized model files...\")\n",
    "    \n",
    "    files.download('crypto_bert_optimized.h5')\n",
    "    files.download('tokenizer_optimized.json')\n",
    "    files.download('config_optimized.json')\n",
    "    \n",
    "    print(\"‚úÖ Download complete!\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è Files saved locally (not in Colab)\")\n",
    "\n",
    "print(\"\\nüéä All done! Your optimized Crypto-BERT is ready for integration with your other models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usage-notes"
   },
   "source": [
    "## üìù Usage Notes\n",
    "\n",
    "### Key Optimizations Made:\n",
    "\n",
    "1. **Reduced Model Size**:\n",
    "   - Hidden size: 768 ‚Üí 384\n",
    "   - Layers: 12 ‚Üí 6\n",
    "   - Sequence length: 512 ‚Üí 256\n",
    "   - Vocab size: 30K ‚Üí 15K\n",
    "\n",
    "2. **Memory Efficiency**:\n",
    "   - Mixed precision training (float16)\n",
    "   - Memory growth enabled\n",
    "   - Batch processing\n",
    "   - Garbage collection\n",
    "\n",
    "3. **Training Speed**:\n",
    "   - Larger batch size (32)\n",
    "   - Fewer epochs (5)\n",
    "   - Simplified data generation\n",
    "   - Efficient tokenization\n",
    "\n",
    "### Integration with Your System:\n",
    "\n",
    "```python\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('crypto_bert_optimized.h5')\n",
    "\n",
    "# Load tokenizer\n",
    "with open('tokenizer_optimized.json', 'r') as f:\n",
    "    tokenizer_data = json.load(f)\n",
    "\n",
    "# Use for predictions in your ensemble\n",
    "predictions = model.predict(encoded_sequences)\n",
    "```\n",
    "\n",
    "This optimized version should train much faster on Colab while still providing good accuracy for your Wi-Fi vulnerability detection system!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}